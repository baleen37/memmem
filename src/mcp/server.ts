/**
 * MCP Server for Conversation Memory.
 *
 * Simplified 3-tool architecture:
 * 1. search - Single query string, returns compact observations
 * 2. get_observations - Full details by ID array
 * 3. read - Raw conversation from JSONL
 *
 * Progressive disclosure:
 * - Layer 1: search() returns compact observations (~30t)
 * - Layer 2: get_observations() returns full details (~200-500t)
 * - Layer 3: read() returns raw conversation (~500-2000t)
 */

import { Server } from '@modelcontextprotocol/sdk/server/index.js';
import { StdioServerTransport } from '@modelcontextprotocol/sdk/server/stdio.js';
import {
  CallToolRequestSchema,
  ListToolsRequestSchema,
} from '@modelcontextprotocol/sdk/types.js';
import { z } from 'zod';
import type Database from 'better-sqlite3';
import { search } from '../core/search.js';
import { findByIds as getObservationsByIds } from '../core/observations.js';
import { readConversation } from '../core/read.js';
import { openDatabase } from '../core/db.js';
import { loadConfig, createProvider } from '../core/llm/index.js';
import type { LLMConfig, LLMProvider } from '../core/llm/index.js';
import { logDebug } from '../core/logger.js';

// Zod Schemas for Input Validation

const SearchInputSchema = z
  .object({
    query: z
      .string()
      .min(2, 'Query must be at least 2 characters')
      .describe('Search query string'),
    limit: z
      .number()
      .int()
      .min(1)
      .max(50)
      .default(10)
      .describe('Maximum number of results to return (default: 10)'),
    after: z
      .string()
      .regex(/^\d{4}-\d{2}-\d{2}$/, 'Date must be in YYYY-MM-DD format')
      .optional()
      .describe('Only return results after this date (YYYY-MM-DD format)'),
    before: z
      .string()
      .regex(/^\d{4}-\d{2}-\d{2}$/, 'Date must be in YYYY-MM-DD format')
      .optional()
      .describe('Only return results before this date (YYYY-MM-DD format)'),
    projects: z
      .array(z.string().min(1))
      .optional()
      .describe('Filter results to specific project names'),
    files: z
      .array(z.string().min(1))
      .optional()
      .describe('Filter results to specific file paths'),
  })
  .strict();

export type SearchInput = z.infer<typeof SearchInputSchema>;

const GetObservationsInputSchema = z
  .object({
    ids: z
      .array(z.union([z.string(), z.number()]))
      .min(1, 'Must provide at least 1 observation ID')
      .max(20, 'Cannot get more than 20 observations at once')
      .describe('Array of observation IDs to retrieve'),
    includeOriginal: z
      .boolean()
      .default(false)
      .describe('Include original-language/source text (content_original) when available'),
  })
  .strict();

export type GetObservationsInput = z.infer<typeof GetObservationsInputSchema>;

const ReadInputSchema = z
  .object({
    path: z
      .string()
      .min(1, 'Path is required')
      .describe('Path to the JSONL conversation file'),
    startLine: z
      .number()
      .int()
      .min(1)
      .optional()
      .describe('Starting line number (1-indexed, inclusive)'),
    endLine: z
      .number()
      .int()
      .min(1)
      .optional()
      .describe('Ending line number (1-indexed, inclusive)'),
  })
  .strict();

export type ReadInput = z.infer<typeof ReadInputSchema>;

// Export schemas for testing
export { SearchInputSchema, GetObservationsInputSchema, ReadInputSchema };

// Error Handling Utility

export function handleError(error: unknown): string {
  if (error instanceof Error) {
    return `Error: ${error.message}`;
  }
  return `Error: ${String(error)}`;
}

// Handler Functions (exported for testing)

export interface SearchResult {
  id: string;
  title: string;
  project: string;
  timestamp: number;
}

let cachedQueryNormalizerProvider: LLMProvider | undefined;
let cachedQueryNormalizerConfigKey: string | null = null;
let inFlightQueryNormalizerProvider: Promise<LLMProvider | undefined> | null = null;
let inFlightQueryNormalizerConfigKey: string | null = null;

export function __resetQueryNormalizerCacheForTests(): void {
  cachedQueryNormalizerProvider = undefined;
  cachedQueryNormalizerConfigKey = null;
  inFlightQueryNormalizerProvider = null;
  inFlightQueryNormalizerConfigKey = null;
}

function getConfigCacheKey(config: LLMConfig): string {
  return JSON.stringify([config.provider, config.model, config.apiKey]);
}

async function getQueryNormalizerProvider(): Promise<LLMProvider | undefined> {
  const config = loadConfig();
  if (!config) {
    return undefined;
  }

  const configKey = getConfigCacheKey(config);
  if (cachedQueryNormalizerProvider && cachedQueryNormalizerConfigKey === configKey) {
    return cachedQueryNormalizerProvider;
  }

  if (inFlightQueryNormalizerProvider && inFlightQueryNormalizerConfigKey === configKey) {
    return inFlightQueryNormalizerProvider;
  }

  const providerPromise = createProvider(config)
    .then(provider => {
      cachedQueryNormalizerProvider = provider;
      cachedQueryNormalizerConfigKey = configKey;
      return provider;
    })
    .catch(error => {
      logDebug('handleSearch: query normalizer unavailable, falling back to original query', {
        error: error instanceof Error ? error.message : String(error)
      });
      return undefined;
    })
    .finally(() => {
      if (inFlightQueryNormalizerConfigKey === configKey) {
        inFlightQueryNormalizerProvider = null;
        inFlightQueryNormalizerConfigKey = null;
      }
    });

  inFlightQueryNormalizerProvider = providerPromise;
  inFlightQueryNormalizerConfigKey = configKey;

  return providerPromise;
}

export async function handleSearch(
  params: SearchInput,
  db: Database.Database
): Promise<SearchResult[]> {
  const queryNormalizerProvider = await getQueryNormalizerProvider();

  const results = await search(params.query, {
    db,
    limit: params.limit,
    after: params.after,
    before: params.before,
    projects: params.projects,
    files: params.files,
    queryNormalizerProvider,
  });

  return results.map(r => ({
    id: String(r.id),
    title: r.title,
    project: r.project,
    timestamp: r.timestamp,
  }));
}

export interface ObservationOutput {
  id: number;
  title: string;
  content: string;
  project: string;
  timestamp: number;
  content_original?: string;
}

export async function handleGetObservations(
  params: GetObservationsInput,
  db: Database.Database
): Promise<ObservationOutput[]> {
  // Convert string IDs to numbers
  const numericIds = params.ids.map(id =>
    typeof id === 'string' ? parseInt(id, 10) : id
  );

  const observations = await getObservationsByIds(db, numericIds);

  return observations.map(obs => ({
    id: obs.id,
    title: obs.title,
    content: obs.content,
    project: obs.project,
    timestamp: obs.timestamp,
    ...(params.includeOriginal && obs.contentOriginal ? { content_original: obs.contentOriginal } : {}),
  }));
}

export function handleRead(params: ReadInput): string {
  const result = readConversation(params.path, params.startLine, params.endLine);
  if (result === null) {
    throw new Error(`File not found: ${params.path}`);
  }
  return result;
}

// Create MCP Server

const server = new Server(
  {
    name: 'memmem',
    version: '3.0.0',
  },
  {
    capabilities: {
      tools: {},
    },
  }
);

// Register Tools

server.setRequestHandler(ListToolsRequestSchema, async () => {
  return {
    tools: [
      {
        name: 'search',
        description: `Gives you memory across sessions using observations (structured insights) and conversations. Use BEFORE every task to recover decisions, solutions, and avoid reinventing work. Progressive disclosure: 1) search returns compact observations (~30t), 2) get_observations() for full details (~200-500t), 3) read() for raw conversation (~500-2000t). Internal search strategies: vector_search first, then keyword_search fallback. Supports filters by projects/files and date ranges.`,
        inputSchema: {
          type: 'object',
          properties: {
            query: {
              type: 'string',
              minLength: 2,
              description: 'Search query string'
            },
            limit: {
              type: 'number',
              minimum: 1,
              maximum: 50,
              default: 10,
              description: 'Maximum number of results to return'
            },
            after: {
              type: 'string',
              pattern: '^\\d{4}-\\d{2}-\\d{2}$',
              description: 'Only return results after this date (YYYY-MM-DD format)'
            },
            before: {
              type: 'string',
              pattern: '^\\d{4}-\\d{2}-\\d{2}$',
              description: 'Only return results before this date (YYYY-MM-DD format)'
            },
            projects: {
              type: 'array',
              items: { type: 'string', minLength: 1 },
              description: 'Filter results to specific project names'
            },
            files: {
              type: 'array',
              items: { type: 'string', minLength: 1 },
              description: 'Filter results to specific file paths'
            },
          },
          required: ['query'],
          additionalProperties: false,
        },
        annotations: {
          title: 'Search Conversation Memory',
          readOnlyHint: true,
          destructiveHint: false,
          idempotentHint: true,
          openWorldHint: false,
        },
      },
      {
        name: 'get_observations',
        description: `Get full observation details (Layer 2 of progressive disclosure). Use after search() to retrieve complete information including narrative, facts, concepts, and files. Returns ~200-500 tokens per observation. Essential for understanding the complete context behind decisions and discoveries.`,
        inputSchema: {
          type: 'object',
          properties: {
            ids: {
              type: 'array',
              items: { type: ['string', 'number'] },
              minItems: 1,
              maxItems: 20,
              description: 'Array of observation IDs to retrieve'
            },
            includeOriginal: {
              type: 'boolean',
              default: false,
              description: 'Include original-language/source text (content_original) when available'
            }
          },
          required: ['ids'],
          additionalProperties: false,
        },
        annotations: {
          title: 'Get Full Observation Details',
          readOnlyHint: true,
          destructiveHint: false,
          idempotentHint: true,
          openWorldHint: false,
        },
      },
      {
        name: 'read',
        description: `Returns compressed conversation data from indexed DB (Layer 3 of progressive disclosure). Use to extract detailed context after finding relevant observations with search() and getting full details with get_observations(). Essential for understanding the complete rationale, evolution, and gotchas behind past decisions. Use startLine/endLine pagination for large conversations to avoid context bloat (line numbers are 1-indexed).`,
        inputSchema: {
          type: 'object',
          properties: {
            path: {
              type: 'string',
              minLength: 1,
              description: 'Path to the JSONL conversation file'
            },
            startLine: {
              type: 'number',
              minimum: 1,
              description: 'Starting line number (1-indexed, inclusive)'
            },
            endLine: {
              type: 'number',
              minimum: 1,
              description: 'Ending line number (1-indexed, inclusive)'
            },
          },
          required: ['path'],
          additionalProperties: false,
        },
        annotations: {
          title: 'Show Full Conversation',
          readOnlyHint: true,
          destructiveHint: false,
          idempotentHint: true,
          openWorldHint: false,
        },
      },
    ],
  };
});

// Handle Tool Calls

server.setRequestHandler(CallToolRequestSchema, async (request) => {
  try {
    const { name, arguments: args } = request.params;

    if (name === 'search') {
      const params = SearchInputSchema.parse(args);

      // Open database (persistent storage)
      const db = openDatabase();
      try {
        const results = await handleSearch(params, db);

        // Return compact observations as JSON
        return {
          content: [
            {
              type: 'text',
              text: JSON.stringify({ results }, null, 2),
            },
          ],
        };
      } finally {
        db.close();
      }
    }

    if (name === 'get_observations') {
      const params = GetObservationsInputSchema.parse(args);

      // Open database (persistent storage)
      const db = openDatabase();
      try {
        const observations = await handleGetObservations(params, db);

        if (observations.length === 0) {
          return {
            content: [
              {
                type: 'text',
                text: 'No observations found.',
              },
            ],
          };
        }

        let output = `Retrieved ${observations.length} observation${observations.length > 1 ? 's' : ''}:\n\n`;

        for (const obs of observations) {
          const date = new Date(obs.timestamp).toISOString().split('T')[0];
          const time = new Date(obs.timestamp).toLocaleTimeString('en-US', {
            hour: 'numeric',
            minute: '2-digit',
            hour12: true
          });

          output += `## [${obs.project}, ${date} ${time}] - ${obs.title}\n\n`;
          output += `${obs.content}\n\n`;
          if (params.includeOriginal && obs.content_original) {
            output += `Original: ${obs.content_original}\n\n`;
          }
          output += `---\n\n`;
        }

        return { content: [{ type: 'text', text: output }] };
      } finally {
        db.close();
      }
    }

    if (name === 'read') {
      const params = ReadInputSchema.parse(args);
      const result = handleRead(params);
      return { content: [{ type: 'text', text: result }] };
    }

    throw new Error(`Unknown tool: ${name}`);
  } catch (error) {
    // Return errors within the result (not as protocol errors)
    return {
      content: [
        {
          type: 'text',
          text: handleError(error),
        },
      ],
      isError: true,
    };
  }
});

// Main Function

async function main() {
  console.error('Conversation Memory MCP server running via stdio');

  const transport = new StdioServerTransport();
  await server.connect(transport);
}

// Run the Server

export function shouldRunAsEntrypoint(): boolean {
  return process.env.VITEST !== 'true';
}

if (shouldRunAsEntrypoint()) {
  main().catch((error) => {
    console.error('Server error:', error);
    process.exit(1);
  });
}
